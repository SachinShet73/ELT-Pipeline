This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
airflow/
  dags/
    etl_dag.py
custom_postgres/
  macros/
    film_ratings_macro.sql
    ratings_macro.sql
  models/
    example/
      actors.sql
      film_actors.sql
      film_ratings.sql
      films.sql
      schema.yml
      sources.yml
      specific_movie.sql
  .gitignore
  dbt_project.yml
  README.md
elt/
  Dockerfile
  elt_script.py
source_db_init/
  init.sql
docker-compose.yml
Dockerfile
start.sh
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="airflow/dags/etl_dag.py">
from datetime import datetime, timedelta
from airflow import DAG
from docker.types import Mount
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator
from airflow.providers.docker.operators.docker import DockerOperator
import subprocess

default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'email_on_failure': False,
    'email_on_retry': False,
}

def run_elt_script():
    script_path = "/opt/airflow/elt/elt_script.py"
    result = subprocess.run(["python", script_path], capture_output=True, text=True)

    if result.returncode != 0:
        raise Exception(f"Script failed with error: {result.stderr}")
    else:
        print(result.stdout)

dag = DAG(
    'elt_and_dbt',
    default_args=default_args,
    description='An ELT workflow with dbt',
    start_date=datetime(2025, 8, 19),
    catchup=False
)

t1 = PythonOperator(
    task_id="run_elt_script",
    python_callable=run_elt_script,
    dag=dag
)

t2 = DockerOperator(
    task_id="dbt_run",
    image='ghcr.io/dbt-labs/dbt-postgres:1.4.7',
    command=[
        "run",
        "--profiles-dir",
        "/root",
        "--project-dir",
        "/dbt"
    ],
    auto_remove=True,
    docker_url='unix://var/run/docker.sock',
    network_mode="bridge",
    mounts=[
        Mount(source='/Users/sachinshet/Desktop/ELT-Pipeline/custom_postgres',
              target='/dbt', type='bind'),
        Mount(source='/Users/sachinshet/Desktop/ELT-Pipeline/custom_postgres',
              target='/root', type='bind') 
    ],
    dag=dag
)

t1 >> t2
</file>

<file path="custom_postgres/macros/ratings_macro.sql">
{% macro generate_ratings() %}
        CASE 
            WHEN user_rating >= 4.5 THEN 'Excellent'
            WHEN user_rating >= 4.0 THEN 'Good'
            WHEN user_rating >= 3.0 THEN 'Average'
            ELSE 'Poor'
        END as rating_category
{% endmacro %}
</file>

<file path="custom_postgres/models/example/specific_movie.sql">
{% set film_title = 'Dunkirk' %}

SELECT *
FROM {{ ref('films')}}
WHERE title = '{{ film_title }}'
</file>

<file path="custom_postgres/.gitignore">
target/
dbt_packages/
logs/
</file>

<file path="custom_postgres/README.md">
Welcome to your new dbt project!

### Using the starter project

Try running the following commands:
- dbt run
- dbt test


### Resources:
- Learn more about dbt [in the docs](https://docs.getdbt.com/docs/introduction)
- Check out [Discourse](https://discourse.getdbt.com/) for commonly asked questions and answers
- Join the [chat](https://community.getdbt.com/) on Slack for live discussions and support
- Find [dbt events](https://events.getdbt.com) near you
- Check out [the blog](https://blog.getdbt.com/) for the latest news on dbt's development and best practices
</file>

<file path="start.sh">
cron & 

python /app/elt_script.py
</file>

<file path="custom_postgres/macros/film_ratings_macro.sql">
{% macro generate_film_ratings() %}
WITH film_with_ratings AS (
    SELECT
        film_id,
        title,
        release_date,
        price,
        rating,
        user_rating,
        {{ generate_ratings() }}
    FROM {{ ref('films') }}
),

films_with_actors AS (
    SELECT
        f.film_id,
        f.title,
        STRING_AGG(a.actor_name, ',') as actors
    FROM {{ ref('films') }} f 
    LEFT JOIN {{ ref('film_actors') }} fa ON f.film_id = fa.film_id
    LEFT JOIN {{ ref('actors') }} a ON fa.actor_id = a.actor_id
    GROUP BY f.film_id, f.title
)

SELECT
    fwr.*,
    fwa.actors
FROM film_with_ratings fwr
LEFT JOIN films_with_actors fwa ON fwr.film_id = fwa.film_id
{% endmacro %}
</file>

<file path="elt/elt_script.py">
import subprocess
import time


def wait_for_postgres(host, max_retries=5, delay_seconds=5):
    """Wait for PostgreSQL to become available."""
    retries = 0
    while retries < max_retries:
        try:
            result = subprocess.run(
                ["pg_isready", "-h", host], check=True, capture_output=True, text=True)
            if "accepting connections" in result.stdout:
                print("Successfully connected to PostgreSQL!")
                return True
        except subprocess.CalledProcessError as e:
            print(f"Error connecting to PostgreSQL: {e}")
            retries += 1
            print(
                f"Retrying in {delay_seconds} seconds... (Attempt {retries}/{max_retries})")
            time.sleep(delay_seconds)
    print("Max retries reached. Exiting.")
    return False


# Use the function before running the ELT process
if not wait_for_postgres(host="source_postgres"):
    exit(1)

print("Starting ELT script...")

# Configuration for the source PostgreSQL database
source_config = {
    'dbname': 'source_db',
    'user': 'postgres',
    'password': 'secret',
    # Use the service name from docker-compose as the hostname
    'host': 'source_postgres'
}

# Configuration for the destination PostgreSQL database
destination_config = {
    'dbname': 'destination_db',
    'user': 'postgres',
    'password': 'secret',
    # Use the service name from docker-compose as the hostname
    'host': 'destination_postgres'
}

# Use pg_dump to dump the source database to a SQL file
dump_command = [
    'pg_dump',
    '-h', source_config['host'],
    '-U', source_config['user'],
    '-d', source_config['dbname'],
    '-f', 'data_dump.sql',
    '-w'  # Do not prompt for password
]

# Set the PGPASSWORD environment variable to avoid password prompt
subprocess_env = dict(PGPASSWORD=source_config['password'])

# Execute the dump command
subprocess.run(dump_command, env=subprocess_env, check=True)

# Use psql to load the dumped SQL file into the destination database
load_command = [
    'psql',
    '-h', destination_config['host'],
    '-U', destination_config['user'],
    '-d', destination_config['dbname'],
    '-a', '-f', 'data_dump.sql'
]

# Set the PGPASSWORD environment variable for the destination database
subprocess_env = dict(PGPASSWORD=destination_config['password'])

# Execute the load command
subprocess.run(load_command, env=subprocess_env, check=True)

print("Ending ELT script...")
</file>

<file path="source_db_init/init.sql">
CREATE TABLE users (
    id SERIAL PRIMARY KEY,
    first_name VARCHAR(50),
    last_name VARCHAR(50),
    email VARCHAR(100),
    date_of_birth DATE
);

INSERT INTO users (first_name, last_name, email, date_of_birth) VALUES
('John', 'Doe', 'john.doe@example.com', '1990-01-01'),
('Jane', 'Smith', 'jane.smith@example.com', '1992-05-15'),
('Alice', 'Johnson', 'alice.johnson@example.com', '1985-10-20'),
('Bob', 'Williams', 'bob.williams@example.com', '1998-07-30'),
('Emily', 'Clark', 'emily.clark@example.com', '1987-02-14'),
('Michael', 'Robinson', 'michael.robinson@example.com', '1995-06-05'),
('Sarah', 'Lewis', 'sarah.lewis@example.com', '1989-03-25'),
('David', 'Walker', 'david.walker@example.com', '1992-11-12'),
('Sophia', 'Hall', 'sophia.hall@example.com', '1996-08-08'),
('James', 'Allen', 'james.allen@example.com', '1984-04-20'),
('Olivia', 'Young', 'olivia.young@example.com', '1993-12-30'),
('Chris', 'King', 'chris.king@example.com', '1990-09-15'),
('Grace', 'Wright', 'grace.wright@example.com', '1997-05-10'),
('William', 'Scott', 'william.scott@example.com', '1986-07-22');

CREATE TABLE films (
    film_id SERIAL PRIMARY KEY,
    title VARCHAR(255) NOT NULL,
    release_date DATE,
    price DECIMAL(5,2),
    rating VARCHAR(10),
    user_rating DECIMAL(2,1) CHECK (user_rating >= 1 AND user_rating <= 5)
);

INSERT INTO films (title, release_date, price, rating, user_rating) VALUES
('Inception', '2010-07-16', 12.99, 'PG-13', 4.8),
('The Shawshank Redemption', '1994-09-23', 9.99, 'R', 4.9),
('The Godfather', '1972-03-24', 14.99, 'R', 4.7),
('The Dark Knight', '2008-07-18', 11.99, 'PG-13', 4.8),
('Pulp Fiction', '1994-10-14', 10.99, 'R', 4.6),
('The Matrix', '1999-03-31', 9.99, 'R', 4.7),
('Forrest Gump', '1994-07-06', 8.99, 'PG-13', 4.5),
('Toy Story', '1995-11-22', 7.99, 'G', 4.4),
('Jurassic Park', '1993-06-11', 9.99, 'PG-13', 4.3),
('Avatar', '2009-12-18', 12.99, 'PG-13', 4.2),
('Blade Runner 2049', '2017-10-06', 13.99, 'R', 4.3),
('Mad Max: Fury Road', '2015-05-15', 11.99, 'R', 4.6),
('Coco', '2017-11-22', 9.99, 'PG', 4.8),
('Dunkirk', '2017-07-21', 12.99, 'PG-13', 4.5),
('Spider-Man: Into the Spider-Verse', '2018-12-14', 10.99, 'PG', 4.9),
('Parasite', '2019-10-11', 14.99, 'R', 4.6),
('Whiplash', '2014-10-10', 9.99, 'R', 4.7),
('Inside Out', '2015-06-19', 9.99, 'PG', 4.8),
('The Grand Budapest Hotel', '2014-03-28', 10.99, 'R', 4.4),
('La La Land', '2016-12-09', 11.99, 'PG-13', 4.5);

CREATE TABLE film_category (
    category_id SERIAL PRIMARY KEY,
    film_id INTEGER REFERENCES films(film_id),
    category_name VARCHAR(50) NOT NULL
);

INSERT INTO film_category (film_id, category_name) VALUES
(1, 'Sci-Fi'),
(1, 'Thriller'),
(2, 'Drama'),
(3, 'Crime'),
(3, 'Drama'),
(4, 'Action'),
(4, 'Thriller'),
(5, 'Crime'),
(5, 'Drama'),
(6, 'Sci-Fi'),
(6, 'Action'),
(7, 'Drama'),
(7, 'Romance'),
(8, 'Animation'),
(8, 'Family'),
(9, 'Action'),
(9, 'Adventure'),
(10, 'Sci-Fi'),
(10, 'Adventure'),
(11, 'Sci-Fi'),
(11, 'Drama'),
(12, 'Action'),
(12, 'Adventure'),
(13, 'Animation'),
(13, 'Family'),
(14, 'War'),
(14, 'Drama'),
(15, 'Animation'),
(15, 'Action'),
(16, 'Drama'),
(16, 'Thriller'),
(17, 'Drama'),
(17, 'Music'),
(18, 'Animation'),
(18, 'Family'),
(19, 'Comedy'),
(19, 'Drama'),
(20, 'Drama'),
(20, 'Music');

CREATE TABLE actors (
    actor_id SERIAL PRIMARY KEY,
    actor_name VARCHAR(255) NOT NULL
);

CREATE TABLE film_actors (
    film_id INTEGER REFERENCES films(film_id),
    actor_id INTEGER REFERENCES actors(actor_id),
    PRIMARY KEY (film_id, actor_id)
);

INSERT INTO actors (actor_name) VALUES
('Leonardo DiCaprio'),  -- Associated with Inception
('Tim Robbins'),        -- Associated with The Shawshank Redemption
('Marlon Brando'),      -- Associated with The Godfather
('Christian Bale'),     -- Associated with The Dark Knight
('John Travolta'),      -- Associated with Pulp Fiction
('Keanu Reeves'),       -- Associated with The Matrix
('Tom Hanks'),          -- Associated with Forrest Gump
('Tom Hanks'),          -- Associated with Toy Story (Tom Hanks appears twice for demonstration purposes)
('Sam Neill'),          -- Associated with Jurassic Park
('Sam Worthington'),    -- Associated with Avatar
('Ryan Gosling'),       -- Associated with Blade Runner 2049
('Tom Hardy'),          -- Associated with Mad Max: Fury Road
('Anthony Gonzalez'),   -- Associated with Coco
('Fionn Whitehead'),    -- Associated with Dunkirk
('Shameik Moore'),      -- Associated with Spider-Man: Into the Spider-Verse
('Song Kang-ho'),       -- Associated with Parasite
('Miles Teller'),       -- Associated with Whiplash
('Amy Poehler'),        -- Associated with Inside Out
('Ralph Fiennes'),      -- Associated with The Grand Budapest Hotel
('Emma Stone');         -- Associated with La La Land

INSERT INTO film_actors (film_id, actor_id) VALUES
(1, 1),
(2, 2),
(3, 3),
(4, 4),
(5, 5),
(6, 6),
(7, 7),
(8, 8),
(9, 9),
(10, 10),
(11, 11),
(12, 12),
(13, 13),
(14, 14),
(15, 15),
(16, 16),
(17, 17),
(18, 18),
(19, 19),
(20, 20);
</file>

<file path="custom_postgres/models/example/actors.sql">
SELECT * FROM  {{ source('destination_db','actors') }}
</file>

<file path="custom_postgres/models/example/film_actors.sql">
SELECT * FROM  {{ source('destination_db','film_actors') }}
</file>

<file path="custom_postgres/models/example/film_ratings.sql">
{{ generate_film_ratings() }}
</file>

<file path="custom_postgres/models/example/films.sql">
SELECT * FROM  {{ source('destination_db','films') }}
</file>

<file path="custom_postgres/models/example/schema.yml">
version: 2

models:
  - name: films
    description: "This table contains details about films."
    columns:
      - name: film_id
        description: "Unique identifier for the film."
        tests:
          - unique
          - not_null
      - name: title
        description: "Title of the film."
        tests:
          - not_null
      - name: release_date
        description: "Date the film was released."
        tests:
          - not_null
      - name: price
        description: "Price to buy a ticket."
        tests:
          - not_null
      - name: rating
        description: "The MPAA rating of the film."
        tests:
          - not_null
      - name: user_rating
        description: "Rating that users gave the film."
        tests:
          - not_null
      

  - name: actors
    description: "This table contains details about actors."
    columns:
      - name: actor_id
        description: "Unique identifier for the actor."
        tests:
          - unique
          - not_null
      - name: name
        description: "First name of the actor."
        tests:


  - name: film_actors
    description: "This table maps films to the actors that starred in them."
    columns:
      - name: film_id
        description: "Identifier for the film."
        tests:
          - not_null
      - name: actor_id
        description: "Identifier for the actor."
        tests:
          - not_null
  
  
  - name: film_ratings
    description: "This model aggregates film data and calculates the average rating for each film."
    columns:
      - name: film_id
        description: "The unique identifier for each film."
        tests:
          - unique
          - not_null

      - name: title
        description: "The title of the film."
        tests:
          - not_null

      - name: release_date
        description: "The release date of the film."
        tests:
          - not_null

      - name: price
        description: "The price of the film."
        tests:
          - not_null

      - name: rating
        description: "The MPAA rating of the film."

      - name: user_rating
        description: "The average user rating for the film."
        tests:
          - not_null

      - name: actor_count
        description: "The number of actors in the film."
        tests:
          - not_null

      - name: avg_actor_rating
        description: "The average rating of films that the actors of this film have acted in."
        tests:
          - not_null
</file>

<file path="custom_postgres/models/example/sources.yml">
version: 2

sources:
  - name: destination_db
    description: "Raw data from the destination PostgreSQL database."
    database: destination_db
    schema: public
    tables:
      - name: films
        description: "List of films."
      - name: actors
        description: "List of actors."
      - name: film_actors
        description: "Mapping between films and actors."
</file>

<file path="Dockerfile">
FROM apache/airflow:2.7.0

RUN pip install apache-airflow-providers-docker=3.7.0
</file>

<file path="custom_postgres/dbt_project.yml">
# Name your project! Project names should contain only lowercase characters
# and underscores. A good package name should reflect your organization's
# name or the intended use of these models
name: 'custom_postgres'
version: '1.0.0'
config-version: 2

# This setting configures which "profile" dbt uses for this project.
profile: 'custom_postgres'

# These configurations specify where dbt should look for different types of files.
# The `model-paths` config, for example, states that models in this project can be
# found in the "models/" directory. You probably won't need to change these!
model-paths: ["models"]
analysis-paths: ["analyses"]
test-paths: ["tests"]
seed-paths: ["seeds"]
macro-paths: ["macros"]
snapshot-paths: ["snapshots"]

clean-targets:         # directories to be removed by `dbt clean`
  - "target"
  - "dbt_packages"


# Configuring models
# Full documentation: https://docs.getdbt.com/docs/configuring-models

# In this example config, we tell dbt to build all models in the example/
# directory as views. These settings can be overridden in the individual model
# files using the `{{ config(...) }}` macro.
models:
  custom_postgres:
    # Config indicated by + and applies to all files under models/example/
    example:
      +materialized: table
</file>

<file path="elt/Dockerfile">
FROM python:3.8-slim

# Install PostgreSQL 17 client tools
RUN apt-get update && apt-get install -y postgresql-client cron

COPY start.sh /app/start.sh
COPY elt_script.py .

WORKDIR /app
RUN echo "0 3 *  *  * python /app/elt_script.py" | crontab -

CMD ["python", "elt_script.py"]
</file>

<file path="docker-compose.yml">
version: '3'

services:
  source_postgres:
    image: postgres:15
    ports:
      - "5433:5432"
    networks:
      - elt_network
    environment:
      POSTGRES_DB: source_db
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: secret
    volumes:
      
       - ./source_db_init/init.sql:/docker-entrypoint-initdb.d/init.sql

  destination_postgres:
    image: postgres:15
    ports:
      - "5434:5432"
    networks:
      - elt_network
    environment:
      POSTGRES_DB: destination_db
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: secret

  # elt_script:
  #   build:
  #     context: ./elt
  #     dockerfile: Dockerfile
  #   command: ["python", "elt_script.py"]
  #   networks:
  #     - elt_network
  #   depends_on:
  #     - source_postgres
  #     - destination_postgres

  # dbt:
  #   image: ghcr.io/dbt-labs/dbt-postgres:1.4.7
  #   command:
  #     [
  #       "run",
  #       "--profiles-dir",
  #       "/root/.dbt",
  #       "--project-dir",
  #       "/dbt"
  #     ]
  #   volumes:
  #     - ./custom_postgres:/dbt      
  #     - ~/.dbt:/root/.dbt             
  #   networks:
  #     - elt_network
  #   depends_on:
  #     - elt_script 
  #   environment:
  #      DBT_PROFILE : default
  #      DBT_TARGET : dev

  postgres:
    image: postgres:15
    networks:
      - elt_network
    environment:
      - POSTGRES_USER=airflow
      - POSTGRES_PASSWORD=airflow
      - POSTGRES_DB=airflow

  init-airflow:
    image: apache/airflow:2.7.0
    depends_on:
      - postgres
    networks:
      - elt_network
    environment:
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
    command: >
      bash -c "airflow db reset --yes &&
             airflow db migrate &&
             airflow users create --username airflow --password password --firstname Sachin --lastname Shet --role Admin --email sachin@example.com"
  webserver:
    build: 
      context: .
      dockerfile: Dockerfile
    user: root
    depends_on:
      - postgres
    networks:
      - elt_network
    extra_hosts:
      - "host.docker.internal:host-gateway"
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./elt:/opt/airflow/elt
      - ./custom_postgres:/opt/dbt
      - ~/.dbt:/root/.dbt
      - /var/run/docker.sock:/var/run/docker.sock
    environment:
      - LOAD_EX=n
      - EXECUTOR=Local
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
      - AIRFLOW_CORE_FERNET_KEY=mjWiVpufCtoTLmthnugkJ0xhRBb9MUlBqmvjIuJi3Lc=
      - AIRFLOW__WEBSERVER__DEFAULT_USER_USERNAME=airflow
      - AIRFLOW__WEBSERVER__DEFAULT_USER_PASSWORD=password
      - AIRFLOW_WWW_USER_USERNAME=airflow
      - AIRFLOW__WWW__DEFAULT_USER_PASSWORD=password
    ports:
      - "8080:8080"
    command: standalone

  scheduler:
    build: 
      context: .
      dockerfile: Dockerfile
    user: root
    depends_on:
      - init-airflow
    networks:
      - elt_network
    extra_hosts:
      - "host.docker.internal:host-gateway"
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./elt:/opt/airflow/elt
      - ./custom_postgres:/opt/dbt
      - ~/.dbt:/root/.dbt
      - /var/run/docker.sock:/var/run/docker.sock
    environment:
      - LOAD_EX=n
      - EXECUTOR=Local
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
      - AIRFLOW_CORE_FERNET_KEY=mjWiVpufCtoTLmthnugkJ0xhRBb9MUlBqmvjIuJi3Lc=
      - AIRFLOW__WEBSERVER__DEFAULT_USER_USERNAME=airflow
      - AIRFLOW__WEBSERVER__DEFAULT_USER_PASSWORD=password
      - AIRFLOW_WWW_USER_USERNAME=airflow
      - AIRFLOW__WWW__DEFAULT_USER_PASSWORD=secret
    command: scheduler

networks:
  elt_network:
    driver: bridge
</file>

</files>
